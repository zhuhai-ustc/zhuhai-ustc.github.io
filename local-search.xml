<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>TextCNN</title>
    <link href="/2023/03/03/TextCNN/"/>
    <url>/2023/03/03/TextCNN/</url>
    
    <content type="html"><![CDATA[<h1 id="TextCNN-代码＋图文对应解释"><a href="#TextCNN-代码＋图文对应解释" class="headerlink" title="TextCNN 代码＋图文对应解释"></a>TextCNN 代码＋图文对应解释</h1><h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><p>适合了解TextCNN大体思路，但是对Pytorch不了解的小伙伴阅读</p><h2 id="建议-遇到不懂的函数如squeeze-和unsqueeze-查阅官方文档，或者单独在百度上搜搜这个函数，你就懂了-如图"><a href="#建议-遇到不懂的函数如squeeze-和unsqueeze-查阅官方文档，或者单独在百度上搜搜这个函数，你就懂了-如图" class="headerlink" title="建议:遇到不懂的函数如squeeze()和unsqueeze(),查阅官方文档，或者单独在百度上搜搜这个函数，你就懂了,如图"></a>建议:遇到不懂的函数如squeeze()和unsqueeze(),查阅<a href="https://pytorch.org/docs/stable/search.html?q=squeeze&check_keywords=yes&area=default">官方文档</a>，或者单独在百度上搜搜这个函数，你就懂了,如图</h2><p><img src="https://img-blog.csdnimg.cn/20201216162011764.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h2><p><img src="https://img-blog.csdnimg.cn/20201216162359773.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="模型代码"><a href="#模型代码" class="headerlink" title="模型代码"></a>模型代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CNN</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(CNN,self).__init__()<br>        <span class="hljs-comment"># 设置embeding层</span><br>        self.embedding_choice=embedding_choice<br>        <span class="hljs-keyword">if</span> self.embedding_choice==  <span class="hljs-string">&#x27;rand&#x27;</span>:<br>            self.embedding=nn.Embedding(num_embeddings,embedding_dim)<br>        <span class="hljs-keyword">if</span> self.embedding_choice==  <span class="hljs-string">&#x27;glove&#x27;</span>:<br>            self.embedding = nn.Embedding(num_embeddings, embedding_dim, <br>                padding_idx=PAD_INDEX).from_pretrained(TEXT.vocab.vectors, freeze=<span class="hljs-literal">True</span>)<br>       <span class="hljs-comment">#三层卷积层设置 convolution</span><br>        self.conv1 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                               kernel_size=(<span class="hljs-number">3</span>, embedding_dim), padding=(<span class="hljs-number">2</span>,<span class="hljs-number">0</span>))<br>        <br>        self.conv2 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                               kernel_size=(<span class="hljs-number">4</span>, embedding_dim), padding=(<span class="hljs-number">3</span>,<span class="hljs-number">0</span>))<br>        <br>        self.conv3 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                               kernel_size=(<span class="hljs-number">5</span>, embedding_dim), padding=(<span class="hljs-number">4</span>,<span class="hljs-number">0</span>))<br>        <span class="hljs-comment"># 正则化，防止过拟合</span><br>        self.dropout = nn.Dropout(dropout_p)<br>        <span class="hljs-comment">#全连接层 functional connect</span><br>        self.fc = nn.Linear(filters_num * <span class="hljs-number">3</span>, label_num)<br>       <span class="hljs-comment"># 前向传播</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):      <span class="hljs-comment"># (Batch_size, Length) </span><br>        x=self.embedding(x).unsqueeze(<span class="hljs-number">1</span>)      <span class="hljs-comment">#(Batch_size, Length, Dimention) </span><br>                                       <span class="hljs-comment">#(Batch_size, 1, Length, Dimention) </span><br>        <span class="hljs-comment"># Relu函数 提供非线性</span><br>        x1 = F.relu(self.conv1(x)).squeeze(<span class="hljs-number">3</span>)    <span class="hljs-comment">#(Batch_size, filters_num, length+padding, 1) </span><br>                                          <span class="hljs-comment">#(Batch_size, filters_num, length+padding) </span><br>        <span class="hljs-comment"># 池化层-降维</span><br>        x1 = F.max_pool1d(x1, x1.size(<span class="hljs-number">2</span>)).squeeze(<span class="hljs-number">2</span>)  <span class="hljs-comment">#(Batch_size, filters_num, 1)</span><br>                                               <span class="hljs-comment">#(Batch_size, filters_num) </span><br>         <br>        x2 = F.relu(self.conv2(x)).squeeze(<span class="hljs-number">3</span>)  <br>        x2 = F.max_pool1d(x2, x2.size(<span class="hljs-number">2</span>)).squeeze(<span class="hljs-number">2</span>)      <br>        <br>        x3 = F.relu(self.conv3(x)).squeeze(<span class="hljs-number">3</span>)  <br>        x3 = F.max_pool1d(x3, x3.size(<span class="hljs-number">2</span>)).squeeze(<span class="hljs-number">2</span>)      <br>        <br>        <span class="hljs-comment"># 拼接</span><br>        x = torch.cat((x1, x2, x3), dim=<span class="hljs-number">1</span>)  <span class="hljs-comment">#(Batch_size, filters_num *3 )</span><br>        x = self.dropout(x)      <span class="hljs-comment">#(Batch_size, filters_num *3 )</span><br>        out = self.fc(x)       <span class="hljs-comment">#(Batch_size, label_num  )</span><br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><h2 id="设置-Embedding-层"><a href="#设置-Embedding-层" class="headerlink" title="设置 Embedding 层"></a>设置 Embedding 层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">self.embedding_choice=embedding_choice<br>       <span class="hljs-keyword">if</span> self.embedding_choice==  <span class="hljs-string">&#x27;rand&#x27;</span>:<br>           self.embedding=nn.Embedding(num_embeddings,embedding_dim)<br>       <span class="hljs-keyword">if</span> self.embedding_choice==  <span class="hljs-string">&#x27;glove&#x27;</span>:<br>           self.embedding = nn.Embedding(num_embeddings, embedding_dim, <br>               padding_idx=PAD_INDEX).from_pretrained(TEXT.vocab.vectors, freeze=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>其中 <strong>rand模式</strong> 就是每个词向量随机赋值，<strong>glove模式</strong>就是从预训练的glove词向量中选择，如果这个词在glove中没有，就按照均匀分布给他随机赋值，<strong>padding_idx</strong> 参数为指定某行全为0</p><p><img src="https://img-blog.csdnimg.cn/20201216161215215.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="设置卷积层"><a href="#设置卷积层" class="headerlink" title="设置卷积层"></a>设置卷积层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#三层卷积层设置 convolution</span><br> self.conv1 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                        kernel_size=(<span class="hljs-number">3</span>, embedding_dim), padding=(<span class="hljs-number">2</span>,<span class="hljs-number">0</span>))<br> <br> self.conv2 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                        kernel_size=(<span class="hljs-number">4</span>, embedding_dim), padding=(<span class="hljs-number">3</span>,<span class="hljs-number">0</span>))<br> <br> self.conv3 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                        kernel_size=(<span class="hljs-number">5</span>, embedding_dim), padding=(<span class="hljs-number">4</span>,<span class="hljs-number">0</span>))<br></code></pre></td></tr></table></figure><p><strong>padding是在外围补多少0</strong></p><h3 id="如图"><a href="#如图" class="headerlink" title="如图"></a>如图</h3><p><img src="https://img-blog.csdnimg.cn/20201216161305642.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h2><p><img src="https://img-blog.csdnimg.cn/2020121616145068.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">x1 = F.max_pool1d(x1, x1.size(<span class="hljs-number">2</span>)).squeeze(<span class="hljs-number">2</span>)  <span class="hljs-comment">#(Batch_size, filters_num, 1)</span><br>                                               <span class="hljs-comment">#(Batch_size, filters_num) </span><br></code></pre></td></tr></table></figure><h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 拼接</span><br>x = torch.cat((x1, x2, x3), dim=<span class="hljs-number">1</span>)  <span class="hljs-comment">#(Batch_size, filters_num *3 )</span><br>x = self.dropout(x)      <span class="hljs-comment">#(Batch_size, filters_num *3 )</span><br>out = self.fc(x)       <span class="hljs-comment">#(Batch_size, label_num  )</span><br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201216161642843.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch并行训练</title>
    <link href="/2023/03/03/PyTorch%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/"/>
    <url>/2023/03/03/PyTorch%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/</url>
    
    <content type="html"><![CDATA[<h2 id="下面代码实现torch多卡同时训练"><a href="#下面代码实现torch多卡同时训练" class="headerlink" title="下面代码实现torch多卡同时训练"></a>下面代码实现torch多卡同时训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pytorch <span class="hljs-keyword">as</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data.distributed <span class="hljs-keyword">import</span> DistributedSample<br>torch.distributed.init_process_group(backend=<span class="hljs-string">&#x27;nccl&#x27;</span>)<span class="hljs-comment">#初始化并行训练</span><br>n_gpu = torch.cuda.device_count()<span class="hljs-comment">#统计gpu数量</span><br>model = torch.nn.DataParallel(model)<span class="hljs-comment">#多卡部署模型</span><br>data  = TensorDataset(data)<br>data = DistributedSample(data)<span class="hljs-comment">#分布训练</span><br>loss = model(data)<br>loss = loss.mean()<span class="hljs-comment">#多卡取平均loss</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
