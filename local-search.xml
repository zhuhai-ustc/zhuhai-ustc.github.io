<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>常见损失函数总结</title>
    <link href="/2023/03/04/%E5%B8%B8%E8%A7%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93/"/>
    <url>/2023/03/04/%E5%B8%B8%E8%A7%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h1 id="二分类交叉熵损失函数BCELoss"><a href="#二分类交叉熵损失函数BCELoss" class="headerlink" title="二分类交叉熵损失函数BCELoss"></a>二分类交叉熵损失函数BCELoss</h1><h1 id="交叉熵损失函数CrossEntropyLoss"><a href="#交叉熵损失函数CrossEntropyLoss" class="headerlink" title="交叉熵损失函数CrossEntropyLoss"></a>交叉熵损失函数CrossEntropyLoss</h1><h1 id="L1损失函数L1Loss"><a href="#L1损失函数L1Loss" class="headerlink" title="L1损失函数L1Loss"></a>L1损失函数L1Loss</h1><p>也称为Mean Absolute Error，即平均绝对误差(MAE),它衡量的是预测值与真实值之间距离的平均误差幅度，作用范围为0到正无穷。</p><ul><li>优点：无论对于什么样的输入值，都有着稳定的梯度，不会导致梯度爆炸问题，具有较为稳健性的解</li><li>缺点：在中心点是折点，不能求导，梯度下降时要是恰好学习到w=0就没法接着进行了</li></ul><p>$$<br>loss(x,y) = \frac{1}{n}\sum_{i=1}^{n}|x_i-y_i|<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br>x = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">5</span>,requires_grad = <span class="hljs-literal">True</span>)<br>y = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">5</span>)<br>loss = nn.L1Loss()<br>output = loss(x,y) <span class="hljs-comment">#调用函数</span><br><span class="hljs-built_in">print</span>(output) <span class="hljs-comment">#1.024</span><br><br><span class="hljs-comment">#复现</span><br>output_ = torch.mean(torch.<span class="hljs-built_in">abs</span>(x-y)) <span class="hljs-comment"># 1.024</span><br></code></pre></td></tr></table></figure><p>下面是L1Loss的LOSS图：<br><video id="video" controls="" preload="l1loss.mp4" poster="封面"><br>      <source id="mp4" src="l1loss.mp4" type="video/mp4"><br></videos></p><h1 id="L2损失函数-MSE损失函数MSELoss"><a href="#L2损失函数-MSE损失函数MSELoss" class="headerlink" title="L2损失函数/MSE损失函数MSELoss"></a>L2损失函数/MSE损失函数MSELoss</h1><p>均方误差（Mean Square Error,MSE），模型预测值f(x)和样本真实值y之间差值平方的平均值。</p><ul><li>优点：各点都连续光滑，方便求导，具有较为稳定的解；对离群点（Outliers）或者异常值更具有鲁棒性。</li><li>缺点：不是特别的稳健，因为当函数的输入值距离真实值较远的时候，对应loss值很大在两侧，则使用梯度下降法求解的时候梯度很大，可能导致梯度爆炸； 由图可知其在0点处的导数不连续，使得求解效率低下，导致收敛速度慢；而对于较小的损失值，其梯度也同其他区间损失值的梯度一样大，所以不利于网络的学习。<br>$$<br>loss(x,y) = \frac{1}{n}\sum_{i=1}^{n}(x_i-y_i)^2<br>$$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br>x = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">5</span>,requires_grad = <span class="hljs-literal">True</span>)<br>y = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">5</span>)<br>loss = nn.MSELoss()<br>output = loss(x,y) <span class="hljs-comment">#调用函数</span><br><span class="hljs-built_in">print</span>(output) <span class="hljs-comment">#1.9563</span><br><br><span class="hljs-comment">#复现</span><br>output_ = torch.mean(torch.<span class="hljs-built_in">abs</span>(x-y)**<span class="hljs-number">2</span>) <span class="hljs-comment"># 1.9563</span><br></code></pre></td></tr></table></figure><p>下面是L2Loss的LOSS图：<br><video id="video" controls="" preload="l2loss.mp4" poster="封面"><br>      <source id="mp4" src="l2loss.mp4" type="video/mp4"><br></videos></p><h1 id="平滑L1-Smooth-L1-损失函数SmoothL1Loss"><a href="#平滑L1-Smooth-L1-损失函数SmoothL1Loss" class="headerlink" title="平滑L1 (Smooth L1)损失函数SmoothL1Loss"></a>平滑L1 (Smooth L1)损失函数SmoothL1Loss</h1><p>L1的平滑输出，其功能是减轻离群点带来的影响。即平滑的L1损失（SLL）。SLL通过综合L1和L2损失的优点，在0点处附近采用了L2损失中的平方函数，解决了L1损失在0点处梯度不可导的问题，使其更加平滑易于收敛。此外，在|x|&gt;1的区间上，它又采用了L1损失中的线性函数，使得梯度能够快速下降。真实值和预测值差别较小时（绝对值差小于1），梯度也会比较小（损失函数比普通L1 loss在此处更圆滑），可以收敛得更快。真实值和预测值差别较大时，梯度值足够小（普通L2 loss在这种位置梯度值就很大，容易梯度爆炸）</p><!--$$loss(x,y) =  \left\{\begin{aligned} &\frac{1}{2}(x-y)^2  &\quad & if |x-y|<\delta\\ &\delta|x-y|-\frac{1}{2}\delta^2 & \quad & other\\\end{aligned}\right.$$--><p>$$<br>loss(x,y) = \frac{1}{2}(x-y)^2  …… if |x-y|&lt;\delta<br>$$<br>$$<br>loss(x,y) = \delta|x-y|-\frac{1}{2}\delta^2  …… other<br>$$</p><h2 id="三者区别"><a href="#三者区别" class="headerlink" title="三者区别"></a>三者区别</h2><ul><li>（1）L1 loss在零点不平滑，此处不可导，所以在w=0时没法接着梯度下降了，用的少</li><li>（2）L2 loss对离群点比较敏感，离群点处的梯度很大，容易梯度爆炸</li><li>（3）smooth L1 loss结合了L1和L2的优点，修改了零点不平滑问题，且比L2 loss对异常值的鲁棒性更强<h1 id="KL散度KLDivLoss"><a href="#KL散度KLDivLoss" class="headerlink" title="KL散度KLDivLoss"></a>KL散度KLDivLoss</h1></li></ul><h1 id="余弦相似度CosineEmbeddingLoss"><a href="#余弦相似度CosineEmbeddingLoss" class="headerlink" title="余弦相似度CosineEmbeddingLoss"></a>余弦相似度CosineEmbeddingLoss</h1>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch教程</title>
    <link href="/2023/03/04/PyTorch%E6%95%99%E7%A8%8B/"/>
    <url>/2023/03/04/PyTorch%E6%95%99%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="推荐一些比较实用的torch教程"><a href="#推荐一些比较实用的torch教程" class="headerlink" title="推荐一些比较实用的torch教程"></a>推荐一些比较实用的torch教程</h1><ul><li>深入浅出PyTorch <a href="https://github.com/datawhalechina/thorough-pytorch">链接</a></li><li>Pytorch模型训练实用教程 <a href="https://github.com/TingsongYu/PyTorch_Tutorial">链接</a></li><li>包含各个深度学习任务的torch教程 <a href="https://github.com/fendouai/PyTorchDocs">链接</a></li><li>小土堆的教程 <a href="https://github.com/xiaotudui/pytorch-tutorial">链接</a></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>TextCNN</title>
    <link href="/2023/03/03/TextCNN/"/>
    <url>/2023/03/03/TextCNN/</url>
    
    <content type="html"><![CDATA[<h1 id="TextCNN-代码＋图文对应解释"><a href="#TextCNN-代码＋图文对应解释" class="headerlink" title="TextCNN 代码＋图文对应解释"></a>TextCNN 代码＋图文对应解释</h1><h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><p>适合了解TextCNN大体思路，但是对Pytorch不了解的小伙伴阅读</p><h2 id="建议-遇到不懂的函数如squeeze-和unsqueeze-查阅官方文档，或者单独在百度上搜搜这个函数，你就懂了-如图"><a href="#建议-遇到不懂的函数如squeeze-和unsqueeze-查阅官方文档，或者单独在百度上搜搜这个函数，你就懂了-如图" class="headerlink" title="建议:遇到不懂的函数如squeeze()和unsqueeze(),查阅官方文档，或者单独在百度上搜搜这个函数，你就懂了,如图"></a>建议:遇到不懂的函数如squeeze()和unsqueeze(),查阅<a href="https://pytorch.org/docs/stable/search.html?q=squeeze&check_keywords=yes&area=default">官方文档</a>，或者单独在百度上搜搜这个函数，你就懂了,如图</h2><p><img src="https://img-blog.csdnimg.cn/20201216162011764.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h2><p><img src="https://img-blog.csdnimg.cn/20201216162359773.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="模型代码"><a href="#模型代码" class="headerlink" title="模型代码"></a>模型代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CNN</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(CNN,self).__init__()<br>        <span class="hljs-comment"># 设置embeding层</span><br>        self.embedding_choice=embedding_choice<br>        <span class="hljs-keyword">if</span> self.embedding_choice==  <span class="hljs-string">&#x27;rand&#x27;</span>:<br>            self.embedding=nn.Embedding(num_embeddings,embedding_dim)<br>        <span class="hljs-keyword">if</span> self.embedding_choice==  <span class="hljs-string">&#x27;glove&#x27;</span>:<br>            self.embedding = nn.Embedding(num_embeddings, embedding_dim, <br>                padding_idx=PAD_INDEX).from_pretrained(TEXT.vocab.vectors, freeze=<span class="hljs-literal">True</span>)<br>       <span class="hljs-comment">#三层卷积层设置 convolution</span><br>        self.conv1 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                               kernel_size=(<span class="hljs-number">3</span>, embedding_dim), padding=(<span class="hljs-number">2</span>,<span class="hljs-number">0</span>))<br>        <br>        self.conv2 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                               kernel_size=(<span class="hljs-number">4</span>, embedding_dim), padding=(<span class="hljs-number">3</span>,<span class="hljs-number">0</span>))<br>        <br>        self.conv3 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                               kernel_size=(<span class="hljs-number">5</span>, embedding_dim), padding=(<span class="hljs-number">4</span>,<span class="hljs-number">0</span>))<br>        <span class="hljs-comment"># 正则化，防止过拟合</span><br>        self.dropout = nn.Dropout(dropout_p)<br>        <span class="hljs-comment">#全连接层 functional connect</span><br>        self.fc = nn.Linear(filters_num * <span class="hljs-number">3</span>, label_num)<br>       <span class="hljs-comment"># 前向传播</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):      <span class="hljs-comment"># (Batch_size, Length) </span><br>        x=self.embedding(x).unsqueeze(<span class="hljs-number">1</span>)      <span class="hljs-comment">#(Batch_size, Length, Dimention) </span><br>                                       <span class="hljs-comment">#(Batch_size, 1, Length, Dimention) </span><br>        <span class="hljs-comment"># Relu函数 提供非线性</span><br>        x1 = F.relu(self.conv1(x)).squeeze(<span class="hljs-number">3</span>)    <span class="hljs-comment">#(Batch_size, filters_num, length+padding, 1) </span><br>                                          <span class="hljs-comment">#(Batch_size, filters_num, length+padding) </span><br>        <span class="hljs-comment"># 池化层-降维</span><br>        x1 = F.max_pool1d(x1, x1.size(<span class="hljs-number">2</span>)).squeeze(<span class="hljs-number">2</span>)  <span class="hljs-comment">#(Batch_size, filters_num, 1)</span><br>                                               <span class="hljs-comment">#(Batch_size, filters_num) </span><br>         <br>        x2 = F.relu(self.conv2(x)).squeeze(<span class="hljs-number">3</span>)  <br>        x2 = F.max_pool1d(x2, x2.size(<span class="hljs-number">2</span>)).squeeze(<span class="hljs-number">2</span>)      <br>        <br>        x3 = F.relu(self.conv3(x)).squeeze(<span class="hljs-number">3</span>)  <br>        x3 = F.max_pool1d(x3, x3.size(<span class="hljs-number">2</span>)).squeeze(<span class="hljs-number">2</span>)      <br>        <br>        <span class="hljs-comment"># 拼接</span><br>        x = torch.cat((x1, x2, x3), dim=<span class="hljs-number">1</span>)  <span class="hljs-comment">#(Batch_size, filters_num *3 )</span><br>        x = self.dropout(x)      <span class="hljs-comment">#(Batch_size, filters_num *3 )</span><br>        out = self.fc(x)       <span class="hljs-comment">#(Batch_size, label_num  )</span><br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><h2 id="设置-Embedding-层"><a href="#设置-Embedding-层" class="headerlink" title="设置 Embedding 层"></a>设置 Embedding 层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">self.embedding_choice=embedding_choice<br>       <span class="hljs-keyword">if</span> self.embedding_choice==  <span class="hljs-string">&#x27;rand&#x27;</span>:<br>           self.embedding=nn.Embedding(num_embeddings,embedding_dim)<br>       <span class="hljs-keyword">if</span> self.embedding_choice==  <span class="hljs-string">&#x27;glove&#x27;</span>:<br>           self.embedding = nn.Embedding(num_embeddings, embedding_dim, <br>               padding_idx=PAD_INDEX).from_pretrained(TEXT.vocab.vectors, freeze=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>其中 <strong>rand模式</strong> 就是每个词向量随机赋值，<strong>glove模式</strong>就是从预训练的glove词向量中选择，如果这个词在glove中没有，就按照均匀分布给他随机赋值，<strong>padding_idx</strong> 参数为指定某行全为0</p><p><img src="https://img-blog.csdnimg.cn/20201216161215215.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="设置卷积层"><a href="#设置卷积层" class="headerlink" title="设置卷积层"></a>设置卷积层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#三层卷积层设置 convolution</span><br> self.conv1 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                        kernel_size=(<span class="hljs-number">3</span>, embedding_dim), padding=(<span class="hljs-number">2</span>,<span class="hljs-number">0</span>))<br> <br> self.conv2 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                        kernel_size=(<span class="hljs-number">4</span>, embedding_dim), padding=(<span class="hljs-number">3</span>,<span class="hljs-number">0</span>))<br> <br> self.conv3 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                        kernel_size=(<span class="hljs-number">5</span>, embedding_dim), padding=(<span class="hljs-number">4</span>,<span class="hljs-number">0</span>))<br></code></pre></td></tr></table></figure><p><strong>padding是在外围补多少0</strong></p><h3 id="如图"><a href="#如图" class="headerlink" title="如图"></a>如图</h3><p><img src="https://img-blog.csdnimg.cn/20201216161305642.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h2><p><img src="https://img-blog.csdnimg.cn/2020121616145068.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">x1 = F.max_pool1d(x1, x1.size(<span class="hljs-number">2</span>)).squeeze(<span class="hljs-number">2</span>)  <span class="hljs-comment">#(Batch_size, filters_num, 1)</span><br>                                               <span class="hljs-comment">#(Batch_size, filters_num) </span><br></code></pre></td></tr></table></figure><h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 拼接</span><br>x = torch.cat((x1, x2, x3), dim=<span class="hljs-number">1</span>)  <span class="hljs-comment">#(Batch_size, filters_num *3 )</span><br>x = self.dropout(x)      <span class="hljs-comment">#(Batch_size, filters_num *3 )</span><br>out = self.fc(x)       <span class="hljs-comment">#(Batch_size, label_num  )</span><br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201216161642843.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch并行训练</title>
    <link href="/2023/03/03/PyTorch%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/"/>
    <url>/2023/03/03/PyTorch%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/</url>
    
    <content type="html"><![CDATA[<h2 id="下面代码实现torch多卡同时训练"><a href="#下面代码实现torch多卡同时训练" class="headerlink" title="下面代码实现torch多卡同时训练"></a>下面代码实现torch多卡同时训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pytorch <span class="hljs-keyword">as</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data.distributed <span class="hljs-keyword">import</span> DistributedSample<br>torch.distributed.init_process_group(backend=<span class="hljs-string">&#x27;nccl&#x27;</span>)<span class="hljs-comment">#初始化并行训练</span><br>n_gpu = torch.cuda.device_count()<span class="hljs-comment">#统计gpu数量</span><br>model = torch.nn.DataParallel(model)<span class="hljs-comment">#多卡部署模型</span><br>data  = TensorDataset(data)<br>data = DistributedSample(data)<span class="hljs-comment">#分布训练</span><br>loss = model(data)<br>loss = loss.mean()<span class="hljs-comment">#多卡取平均loss</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
