<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>GPT 各层详解</title>
    <link href="/2023/03/10/GPT-%E5%90%84%E5%B1%82%E8%AF%A6%E8%A7%A3/"/>
    <url>/2023/03/10/GPT-%E5%90%84%E5%B1%82%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="Mask-attention"><a href="#Mask-attention" class="headerlink" title="Mask attention"></a>Mask attention</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GPT2Attention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config, is_cross_attention=<span class="hljs-literal">False</span>, layer_idx=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        max_positions = config.max_position_embeddings<br>        self.register_buffer(<br>            <span class="hljs-string">&quot;bias&quot;</span>,<br>            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8)).view(<br>                <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, max_positions, max_positions<br>            ),<br>        )<br>        self.register_buffer(<span class="hljs-string">&quot;masked_bias&quot;</span>, torch.tensor(-<span class="hljs-number">1e4</span>))<br><br>        self.embed_dim = config.hidden_size<br>        self.num_heads = config.num_attention_heads<br>        self.head_dim = self.embed_dim // self.num_heads<br>        self.split_size = self.embed_dim<br>        <span class="hljs-keyword">if</span> self.head_dim * self.num_heads != self.embed_dim:<br>            <span class="hljs-keyword">raise</span> ValueError(<br>                <span class="hljs-string">f&quot;`embed_dim` must be divisible by num_heads (got `embed_dim`: <span class="hljs-subst">&#123;self.embed_dim&#125;</span> and `num_heads`: <span class="hljs-subst">&#123;self.num_heads&#125;</span>).&quot;</span><br>            )<br><br>        self.scale_attn_weights = config.scale_attn_weights<br>        self.is_cross_attention = is_cross_attention<br><br>        <span class="hljs-comment"># Layer-wise attention scaling, reordering, and upcasting</span><br>        self.scale_attn_by_inverse_layer_idx = config.scale_attn_by_inverse_layer_idx<br>        self.layer_idx = layer_idx<br>        self.reorder_and_upcast_attn = config.reorder_and_upcast_attn<br><br>        <span class="hljs-keyword">if</span> self.is_cross_attention:<br>            self.c_attn = Conv1D(<span class="hljs-number">2</span> * self.embed_dim, self.embed_dim)<br>            self.q_attn = Conv1D(self.embed_dim, self.embed_dim)<br>        <span class="hljs-keyword">else</span>:<br>            self.c_attn = Conv1D(<span class="hljs-number">3</span> * self.embed_dim, self.embed_dim)<br>        self.c_proj = Conv1D(self.embed_dim, self.embed_dim)<br><br>        self.attn_dropout = nn.Dropout(config.attn_pdrop)<br>        self.resid_dropout = nn.Dropout(config.resid_pdrop)<br><br>        self.pruned_heads = <span class="hljs-built_in">set</span>()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">prune_heads</span>(<span class="hljs-params">self, heads</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(heads) == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">return</span><br>        heads, index = find_pruneable_heads_and_indices(heads, self.num_heads, self.head_dim, self.pruned_heads)<br>        index_attn = torch.cat([index, index + self.split_size, index + (<span class="hljs-number">2</span> * self.split_size)])<br><br>        <span class="hljs-comment"># Prune conv1d layers</span><br>        self.c_attn = prune_conv1d_layer(self.c_attn, index_attn, dim=<span class="hljs-number">1</span>)<br>        self.c_proj = prune_conv1d_layer(self.c_proj, index, dim=<span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># Update hyper params</span><br>        self.split_size = (self.split_size // self.num_heads) * (self.num_heads - <span class="hljs-built_in">len</span>(heads))<br>        self.num_heads = self.num_heads - <span class="hljs-built_in">len</span>(heads)<br>        self.pruned_heads = self.pruned_heads.union(heads)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_attn</span>(<span class="hljs-params">self, query, key, value, attention_mask=<span class="hljs-literal">None</span>, head_mask=<span class="hljs-literal">None</span></span>):<br>        attn_weights = torch.matmul(query, key.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>))<br><br>        <span class="hljs-keyword">if</span> self.scale_attn_weights:<br>            attn_weights = attn_weights / (value.size(-<span class="hljs-number">1</span>) ** <span class="hljs-number">0.5</span>)<br><br>        <span class="hljs-comment"># Layer-wise attention scaling</span><br>        <span class="hljs-keyword">if</span> self.scale_attn_by_inverse_layer_idx:<br>            attn_weights = attn_weights / <span class="hljs-built_in">float</span>(self.layer_idx + <span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.is_cross_attention:<br>            <span class="hljs-comment"># if only &quot;normal&quot; attention layer implements causal mask</span><br>            query_length, key_length = query.size(-<span class="hljs-number">2</span>), key.size(-<span class="hljs-number">2</span>)<br>            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].<span class="hljs-built_in">bool</span>()<br>            attn_weights = torch.where(causal_mask, attn_weights, self.masked_bias.to(attn_weights.dtype))<br><br>        <span class="hljs-keyword">if</span> attention_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># Apply the attention mask</span><br>            attn_weights = attn_weights + attention_mask<br><br>        attn_weights = nn.functional.softmax(attn_weights, dim=-<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># Downcast (if necessary) back to V&#x27;s dtype (if in mixed-precision) -- No-Op otherwise</span><br>        attn_weights = attn_weights.<span class="hljs-built_in">type</span>(value.dtype)<br>        attn_weights = self.attn_dropout(attn_weights)<br><br>        <span class="hljs-comment"># Mask heads if we want to</span><br>        <span class="hljs-keyword">if</span> head_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            attn_weights = attn_weights * head_mask<br><br>        attn_output = torch.matmul(attn_weights, value)<br><br>        <span class="hljs-keyword">return</span> attn_output, attn_weights<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_upcast_and_reordered_attn</span>(<span class="hljs-params">self, query, key, value, attention_mask=<span class="hljs-literal">None</span>, head_mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># Use `torch.baddbmm` (a bit more efficient w/ alpha param for scaling -- from Megatron-LM)</span><br>        bsz, num_heads, q_seq_len, dk = query.size()<br>        _, _, k_seq_len, _ = key.size()<br><br>        <span class="hljs-comment"># Preallocate attn_weights for `baddbmm`</span><br>        attn_weights = torch.empty(bsz * num_heads, q_seq_len, k_seq_len, dtype=torch.float32, device=query.device)<br><br>        <span class="hljs-comment"># Compute Scale Factor</span><br>        scale_factor = <span class="hljs-number">1.0</span><br>        <span class="hljs-keyword">if</span> self.scale_attn_weights:<br>            scale_factor /= <span class="hljs-built_in">float</span>(value.size(-<span class="hljs-number">1</span>)) ** <span class="hljs-number">0.5</span><br><br>        <span class="hljs-keyword">if</span> self.scale_attn_by_inverse_layer_idx:<br>            scale_factor /= <span class="hljs-built_in">float</span>(self.layer_idx + <span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># Upcast (turn off autocast) and reorder (Scale K by 1 / root(dk))</span><br>        <span class="hljs-keyword">if</span> is_amp_available:<br>            <span class="hljs-keyword">with</span> autocast(enabled=<span class="hljs-literal">False</span>):<br>                q, k = query.reshape(-<span class="hljs-number">1</span>, q_seq_len, dk), key.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>).reshape(-<span class="hljs-number">1</span>, dk, k_seq_len)<br>                attn_weights = torch.baddbmm(attn_weights, q.<span class="hljs-built_in">float</span>(), k.<span class="hljs-built_in">float</span>(), beta=<span class="hljs-number">0</span>, alpha=scale_factor)<br>                attn_weights = attn_weights.reshape(bsz, num_heads, q_seq_len, k_seq_len)<br>        <span class="hljs-keyword">else</span>:<br>            q, k = query.reshape(-<span class="hljs-number">1</span>, q_seq_len, dk), key.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>).reshape(-<span class="hljs-number">1</span>, dk, k_seq_len)<br>            attn_weights = torch.baddbmm(attn_weights, q.<span class="hljs-built_in">float</span>(), k.<span class="hljs-built_in">float</span>(), beta=<span class="hljs-number">0</span>, alpha=scale_factor)<br>            attn_weights = attn_weights.reshape(bsz, num_heads, q_seq_len, k_seq_len)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.is_cross_attention:<br>            <span class="hljs-comment"># if only &quot;normal&quot; attention layer implements causal mask</span><br>            query_length, key_length = query.size(-<span class="hljs-number">2</span>), key.size(-<span class="hljs-number">2</span>)<br>            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].<span class="hljs-built_in">bool</span>()<br>            attn_weights = torch.where(causal_mask, attn_weights, self.masked_bias.to(attn_weights.dtype))<br><br>        <span class="hljs-keyword">if</span> attention_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># Apply the attention mask</span><br>            attn_weights = attn_weights + attention_mask<br><br>        attn_weights = nn.functional.softmax(attn_weights, dim=-<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># Downcast (if necessary) back to V&#x27;s dtype (if in mixed-precision) -- No-Op if otherwise</span><br>        <span class="hljs-keyword">if</span> attn_weights.dtype != torch.float32:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&quot;Error with upcasting, attn_weights does not have dtype torch.float32&quot;</span>)<br>        attn_weights = attn_weights.<span class="hljs-built_in">type</span>(value.dtype)<br>        attn_weights = self.attn_dropout(attn_weights)<br><br>        <span class="hljs-comment"># Mask heads if we want to</span><br>        <span class="hljs-keyword">if</span> head_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            attn_weights = attn_weights * head_mask<br><br>        attn_output = torch.matmul(attn_weights, value)<br><br>        <span class="hljs-keyword">return</span> attn_output, attn_weights<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_split_heads</span>(<span class="hljs-params">self, tensor, num_heads, attn_head_size</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Splits hidden_size dim into attn_head_size and num_heads</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        new_shape = tensor.size()[:-<span class="hljs-number">1</span>] + (num_heads, attn_head_size)<br>        tensor = tensor.view(new_shape)<br>        <span class="hljs-keyword">return</span> tensor.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>)  <span class="hljs-comment"># (batch, head, seq_length, head_features)</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_merge_heads</span>(<span class="hljs-params">self, tensor, num_heads, attn_head_size</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Merges attn_head_size dim and num_attn_heads dim into hidden_size</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        tensor = tensor.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>).contiguous()<br>        new_shape = tensor.size()[:-<span class="hljs-number">2</span>] + (num_heads * attn_head_size,)<br>        <span class="hljs-keyword">return</span> tensor.view(new_shape)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        hidden_states,</span><br><span class="hljs-params">        layer_past=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        attention_mask=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        head_mask=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        encoder_hidden_states=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        encoder_attention_mask=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        use_cache=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        output_attentions=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-keyword">if</span> encoder_hidden_states <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(self, <span class="hljs-string">&quot;q_attn&quot;</span>):<br>                <span class="hljs-keyword">raise</span> ValueError(<br>                    <span class="hljs-string">&quot;If class is used as cross attention, the weights `q_attn` have to be defined. &quot;</span><br>                    <span class="hljs-string">&quot;Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.&quot;</span><br>                )<br><br>            query = self.q_attn(hidden_states)<br>            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=<span class="hljs-number">2</span>)<br>            attention_mask = encoder_attention_mask<br>        <span class="hljs-keyword">else</span>:<br>            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=<span class="hljs-number">2</span>)<br><br>        query = self._split_heads(query, self.num_heads, self.head_dim)<br>        key = self._split_heads(key, self.num_heads, self.head_dim)<br>        value = self._split_heads(value, self.num_heads, self.head_dim)<br><br>        <span class="hljs-keyword">if</span> layer_past <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            past_key, past_value = layer_past<br>            key = torch.cat((past_key, key), dim=-<span class="hljs-number">2</span>)<br>            value = torch.cat((past_value, value), dim=-<span class="hljs-number">2</span>)<br><br>        <span class="hljs-keyword">if</span> use_cache <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span>:<br>            present = (key, value)<br>        <span class="hljs-keyword">else</span>:<br>            present = <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span> self.reorder_and_upcast_attn:<br>            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)<br>        <span class="hljs-keyword">else</span>:<br>            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)<br><br>        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)<br>        attn_output = self.c_proj(attn_output)<br>        attn_output = self.resid_dropout(attn_output)<br><br>        outputs = (attn_output, present)<br>        <span class="hljs-keyword">if</span> output_attentions:<br>            outputs += (attn_weights,)<br><br>        <span class="hljs-keyword">return</span> outputs  <span class="hljs-comment"># a, present, (attentions)</span><br></code></pre></td></tr></table></figure><h2 id="Cross-attention"><a href="#Cross-attention" class="headerlink" title="Cross attention"></a>Cross attention</h2><h2 id="Generate"><a href="#Generate" class="headerlink" title="Generate"></a>Generate</h2><h3 id="Greedy-search"><a href="#Greedy-search" class="headerlink" title="Greedy search"></a>Greedy search</h3><p>每次选最高的概率词;贪婪搜索的主要缺点是它错过了隐藏在低概率单词后面的高概率单词，如我们上面的草图所示<br><img src="greedy.png" alt="greedy search"></p><h3 id="Beam-search"><a href="#Beam-search" class="headerlink" title="Beam search"></a>Beam search</h3><p>波束搜索将始终找到比贪婪搜索概率更高的输出序列，但不能保证找到最可能的输出。最常见的n-gram惩罚是通过手动将下一个单词的概率设置为0来确保没有n-gram出现两次<br><img src="beam.png" alt="beam search"><br>在开放式一代中，最近提出了波束搜索可能不是最佳选择的几个原因：</p><ul><li>波束搜索可以在期望生成的长度或多或少可预测的任务中很好地工作，如机器翻译或摘要-参见Murray等人（2018）和Yang等人（2018年）。但开放式生成的情况并非如此，其中所需的输出长度可能会有很大变化，例如对话和故事生成。</li><li>我们已经看到，波束搜索严重受到重复生成的影响。在故事生成过程中，这尤其难以用n-gram或其他惩罚来控制，因为在强制“不重复”和重复相同n-gram的循环之间找到一个很好的平衡需要大量的微调。</li><li>正如Ari Holtzman等人（2019）所指出的，高质量的人类语言并不遵循高概率下一个单词的分布。换句话说，作为人类，我们希望生成的文本给我们带来惊喜，而不是无聊/可预测的。作者通过绘制概率图很好地展示了这一点，模型将给出人类文本与波束搜索的区别。</li></ul><h3 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h3><p>通过温度系数和softmax对概率分布进行sharpen<br><img src="sample.png" alt="sampling"></p><h3 id="Top-K-sampling"><a href="#Top-K-sampling" class="headerlink" title="Top-K sampling"></a>Top-K sampling</h3><p>前k个采样意味着按概率排序，并将低于第k个令牌的任何事物的概率归零。它似乎通过去除尾部来提高质量，使其不太可能偏离主题。它不能动态调整从下一个单词概率分布中过滤出来的单词数量;这可能是有问题的，因为有些单词可能来自非常尖锐的分布（上图中右侧的分布），而其他单词则来自更平坦的分布（下图中左侧的分布）。<br><img src="topk.png" alt="Top K"></p><h3 id="Top-p-nucleus-sampling"><a href="#Top-p-nucleus-sampling" class="headerlink" title="Top-p(nucleus) sampling"></a>Top-p(nucleus) sampling</h3><p>与仅从最可能的K个单词中采样不同，在Top-p中，采样从累积概率超过概率p的最小可能单词集合中选择。<br><img src="topp.png" alt="Top p"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>BERT 各层详解</title>
    <link href="/2023/03/09/BERT-embedding%E5%B1%82%E8%AF%A6%E8%A7%A3/"/>
    <url>/2023/03/09/BERT-embedding%E5%B1%82%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="BERT-Embedding"><a href="#BERT-Embedding" class="headerlink" title="BERT Embedding"></a>BERT Embedding</h2><ul><li>segment ID：0/1表示，区分是否是同一句话；第一句话全是0，第二句话全是1；”type_vocab_size”: 2</li><li>token ID:每个单词的索引；”vocab_size”: 21128</li><li>position ID: 绝对embedding；”max_position_embeddings”: 512</li></ul><p>bert embedding  = segment ID + token ID + position ID。 注意：三个ID经过embedding相加</p><h3 id="为什么可以相加？"><a href="#为什么可以相加？" class="headerlink" title="为什么可以相加？"></a>为什么可以相加？</h3><p><img src="embedding.png" alt="embedding"><br>在原始one-hot输入向量上concat，经过变换，最终效果等于对原始向量先token embedding 加position embedding</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertEmbeddings</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)<br>        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)<br>        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)<br>        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)<br>        self.dropout = nn.Dropout(config.hidden_dropout_prob)<br>        self.position_embedding_type = <span class="hljs-built_in">getattr</span>(config, <span class="hljs-string">&quot;position_embedding_type&quot;</span>, <span class="hljs-string">&quot;absolute&quot;</span>)<br>        self.register_buffer(<span class="hljs-string">&quot;position_ids&quot;</span>, torch.arange(config.max_position_embeddings).expand((<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, input_ids=<span class="hljs-literal">None</span>, token_type_ids=<span class="hljs-literal">None</span>, position_ids=<span class="hljs-literal">None</span>, inputs_embeds=<span class="hljs-literal">None</span>, past_key_values_length=<span class="hljs-number">0</span></span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-keyword">if</span> input_ids <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            input_shape = input_ids.size()<br>        <span class="hljs-keyword">else</span>:<br>            input_shape = inputs_embeds.size()[:-<span class="hljs-number">1</span>]<br><br>        seq_length = input_shape[<span class="hljs-number">1</span>]<br><br>        <span class="hljs-keyword">if</span> position_ids <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]   <span class="hljs-comment">##position embedding </span><br><br>       <br>        <span class="hljs-keyword">if</span> token_type_ids <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(self, <span class="hljs-string">&quot;token_type_ids&quot;</span>):<br>                buffered_token_type_ids = self.token_type_ids[:, :seq_length]<br>                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[<span class="hljs-number">0</span>], seq_length)<br>                token_type_ids = buffered_token_type_ids_expanded<br>            <span class="hljs-keyword">else</span>:<br>                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)  <span class="hljs-comment">##token_type_ids embedding </span><br><br>        <span class="hljs-keyword">if</span> inputs_embeds <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            inputs_embeds = self.word_embeddings(input_ids)  <span class="hljs-comment">##input id  embedding </span><br>        token_type_embeddings = self.token_type_embeddings(token_type_ids)<br><br>        embeddings = inputs_embeds + token_type_embeddings<br>        <span class="hljs-keyword">if</span> self.position_embedding_type == <span class="hljs-string">&quot;absolute&quot;</span>:<br>            position_embeddings = self.position_embeddings(position_ids)<br>            embeddings += position_embeddings<br>        embeddings = self.LayerNorm(embeddings)<br>        embeddings = self.dropout(embeddings)  <span class="hljs-comment"># LN+dropout</span><br>        <span class="hljs-keyword">return</span> embeddings<br></code></pre></td></tr></table></figure><h2 id="BERT-Attention"><a href="#BERT-Attention" class="headerlink" title="BERT Attention"></a>BERT Attention</h2><p>$$<br>atten = softmax(\frac{QK^T}{\sqrt{(d_k)})})\cdot V<br>$$<br>其中$d_k = \frac{hidden_size}{num_heads}$；</p><h3 id="为什么除以-d-k"><a href="#为什么除以-d-k" class="headerlink" title="为什么除以$d_k$:"></a>为什么除以$d_k$:</h3><p>如果向量的维度比较大，那么qk点积之后的结果也会比较大，这些数的数量积都会比较大，如果没有经过缩放的话，softmax很有可能就剩下[0, 0, 1]的结果了。为什么非要将均值和方差拉到0和1呢?这是ICS内部协变量偏移问题：机器学习都有一个前提假设那就是数据符合标准正态分布的，当到qk的时候，就发生了变化，因为点积的操作分布就不再是标准正态分布了，也会影响后续所有数据的分布。除以d_k，拉回标准正态分布。由于softmax的马太效应，在求偏导计算梯度的时候，梯度值为0，导致参数无法更新，即梯度消失。经过缩放之后，就不再是0或是1了，梯度值就能够正常的进行参数的更新。   </p><h3 id="为什么多头？"><a href="#为什么多头？" class="headerlink" title="为什么多头？"></a>为什么多头？</h3><p>多头保证了transformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。实验结果好。</p><h3 id="Attention-mask"><a href="#Attention-mask" class="headerlink" title="Attention mask"></a>Attention mask</h3><p>$attention \quad mask = [1,1,1,\cdots,0,0]$</p><h3 id="Padding-mask"><a href="#Padding-mask" class="headerlink" title="Padding mask"></a>Padding mask</h3><p>$padding \quad mask = [1,1,1,\cdots,0,0]$<br>将padding项变成1，其它项变成0,将qk的结果和padding mask相加；相加的时候，padding为1的位置会变成一个很小的数$-10^9$，这样softmax之后为0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertSelfAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config, position_embedding_type=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.num_attention_heads = config.num_attention_heads <span class="hljs-comment">#12 </span><br>        self.attention_head_size = <span class="hljs-built_in">int</span>(config.hidden_size / config.num_attention_heads) <span class="hljs-comment">#768/12 = 64</span><br>        self.all_head_size = self.num_attention_heads * self.attention_head_size<br><br>        self.query = nn.Linear(config.hidden_size, self.all_head_size) <span class="hljs-comment">#Q 768x768</span><br>        self.key = nn.Linear(config.hidden_size, self.all_head_size) <span class="hljs-comment"># K 768x768</span><br>        self.value = nn.Linear(config.hidden_size, self.all_head_size) <span class="hljs-comment">#V 768x768</span><br><br>        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)<br>        self.position_embedding_type = position_embedding_type <span class="hljs-keyword">or</span> <span class="hljs-built_in">getattr</span>(<br>            config, <span class="hljs-string">&quot;position_embedding_type&quot;</span>, <span class="hljs-string">&quot;absolute&quot;</span><br>        )<br>        <span class="hljs-keyword">if</span> self.position_embedding_type == <span class="hljs-string">&quot;relative_key&quot;</span> <span class="hljs-keyword">or</span> self.position_embedding_type == <span class="hljs-string">&quot;relative_key_query&quot;</span>:<br>            self.max_position_embeddings = config.max_position_embeddings<br>            self.distance_embedding = nn.Embedding(<span class="hljs-number">2</span> * config.max_position_embeddings - <span class="hljs-number">1</span>, self.attention_head_size)<br><br>        self.is_decoder = config.is_decoder<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">transpose_for_scores</span>(<span class="hljs-params">self, x</span>):<br>        new_x_shape = x.size()[:-<span class="hljs-number">1</span>] + (self.num_attention_heads, self.attention_head_size)<br>        x = x.view(new_x_shape)<br>        <span class="hljs-keyword">return</span> x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        hidden_states,</span><br><span class="hljs-params">        attention_mask=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        head_mask=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        encoder_hidden_states=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        encoder_attention_mask=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        past_key_value=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        output_attentions=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    </span>):<br>        mixed_query_layer = self.query(hidden_states)<br>        is_cross_attention = encoder_hidden_states <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span> is_cross_attention <span class="hljs-keyword">and</span> past_key_value <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># reuse k,v, cross_attentions</span><br>            key_layer = past_key_value[<span class="hljs-number">0</span>]<br>            value_layer = past_key_value[<span class="hljs-number">1</span>]<br>            attention_mask = encoder_attention_mask<br>        <span class="hljs-keyword">elif</span> is_cross_attention:<br>            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))<br>            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))<br>            attention_mask = encoder_attention_mask<br>        <span class="hljs-keyword">elif</span> past_key_value <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            key_layer = self.transpose_for_scores(self.key(hidden_states))<br>            value_layer = self.transpose_for_scores(self.value(hidden_states))<br>            key_layer = torch.cat([past_key_value[<span class="hljs-number">0</span>], key_layer], dim=<span class="hljs-number">2</span>)<br>            value_layer = torch.cat([past_key_value[<span class="hljs-number">1</span>], value_layer], dim=<span class="hljs-number">2</span>)<br>        <span class="hljs-keyword">else</span>:<br>            key_layer = self.transpose_for_scores(self.key(hidden_states)) <span class="hljs-comment"># b x s x 768 -&gt; b x 12 x s x 64</span><br>            value_layer = self.transpose_for_scores(self.value(hidden_states)) <span class="hljs-comment"># b x s x 768 -&gt; b x 12 x s x 64</span><br><br>        query_layer = self.transpose_for_scores(mixed_query_layer) <span class="hljs-comment"># b x s x 768 -&gt; b x 12 x s x 64</span><br><br>       <br>        <span class="hljs-comment"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span><br>        attention_scores = torch.matmul(query_layer, key_layer.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>)) <span class="hljs-comment">#QK^T b x 12 x s x s</span><br><br><br>        attention_scores = attention_scores / math.sqrt(self.attention_head_size) <span class="hljs-comment"># QK^T/sqrt(64)</span><br>        <span class="hljs-keyword">if</span> attention_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># Apply the attention mask is (precomputed for all layers in BertModel forward() function)</span><br>            attention_scores = attention_scores + attention_mask <span class="hljs-comment"># attention mask</span><br><br>        <span class="hljs-comment"># Normalize the attention scores to probabilities.</span><br>        attention_probs = nn.functional.softmax(attention_scores, dim=-<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># This is actually dropping out entire tokens to attend to, which might</span><br>        <span class="hljs-comment"># seem a bit unusual, but is taken from the original Transformer paper.</span><br>        attention_probs = self.dropout(attention_probs) <span class="hljs-comment">#dropout</span><br><br>        <span class="hljs-comment"># Mask heads if we want to</span><br>        <span class="hljs-keyword">if</span> head_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            attention_probs = attention_probs * head_mask<br><br>        context_layer = torch.matmul(attention_probs, value_layer) <span class="hljs-comment"># * V </span><br><br>        context_layer = context_layer.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>).contiguous() <span class="hljs-comment">#b x 12 x s x 64 -&gt; b x s x 12 x 64</span><br>        new_context_layer_shape = context_layer.size()[:-<span class="hljs-number">2</span>] + (self.all_head_size,)<br>        context_layer = context_layer.view(new_context_layer_shape)<span class="hljs-comment">#b x s x 12 x 64 -&gt; b x s x 768</span><br><br>        outputs = (context_layer, attention_probs) <span class="hljs-keyword">if</span> output_attentions <span class="hljs-keyword">else</span> (context_layer,)<br><br>        <span class="hljs-keyword">if</span> self.is_decoder:<br>            outputs = outputs + (past_key_value,)<br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure><h2 id="BERT-Output"><a href="#BERT-Output" class="headerlink" title="BERT Output"></a>BERT Output</h2><p>集成了linear+dropout+LN</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertSelfOutput</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.dense = nn.Linear(config.hidden_size, config.hidden_size)<br>        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)<br>        self.dropout = nn.Dropout(config.hidden_dropout_prob)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states, input_tensor</span>):<br>        hidden_states = self.dense(hidden_states)<br>        hidden_states = self.dropout(hidden_states)<br>        hidden_states = self.LayerNorm(hidden_states + input_tensor) <span class="hljs-comment">#残差链接</span><br>        <span class="hljs-keyword">return</span> hidden_states<br></code></pre></td></tr></table></figure><h3 id="为什么使用LN"><a href="#为什么使用LN" class="headerlink" title="为什么使用LN"></a>为什么使用LN</h3><p>所以BatchNorm会受到Batch size的影响; 当Batchsize小的时候效果往往不是非常稳定.<br>在nlp领域使用BN效果不好，BN的计算方式是以一个batch中的样本数据去计算它的均值和方差的。<br>这样计算它是有padding的影响的，并且代表不了整个的均值和方差。在刚刚小明和小红例子中，身高这个特征所在的列含义是相同的。但是在nlp里，第一行是”我“字的emb词向量，第二行是”宣“字的emb词向量，经过attention之后，形成的是包含语义信息的向量，每一列代表的含义并不相同了。不能说每个词向量的每个维度代表的含义是一样的。因此从这个⻆度来理解，这里采用BN是不合适的。所以LN用的是比较多的。以每个词向量去做标准化就可以了，这样就不会引入padding不相关的信息。batch_size太小时，一个batch的样本，其均值和方差，不足以代表总体样本的均值与方差。NLP领域不适合用BN</p><h2 id="BERT-Intermediate"><a href="#BERT-Intermediate" class="headerlink" title="BERT Intermediate"></a>BERT Intermediate</h2><p>集成了linear(768-3072)+GELU激活函数<br>$$<br>GELU(x) = 0.5x(1+tanh(\alpha(x+\beta x^3)))<br>$$<br>GELU函数可以能避免梯度消失问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BertIntermediate</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(config.hidden_act, <span class="hljs-built_in">str</span>):<br>            self.intermediate_act_fn = ACT2FN[config.hidden_act]<br>        <span class="hljs-keyword">else</span>:<br>            self.intermediate_act_fn = config.hidden_act<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states</span>):<br>        hidden_states = self.dense(hidden_states)<br>        hidden_states = self.intermediate_act_fn(hidden_states)<br>        <span class="hljs-keyword">return</span> hidden_states<br></code></pre></td></tr></table></figure><h2 id="AdamW"><a href="#AdamW" class="headerlink" title="AdamW"></a>AdamW</h2><p>具体见优化器blog</p><h2 id="wordpiece"><a href="#wordpiece" class="headerlink" title="wordpiece"></a>wordpiece</h2><p>BPE（Byte-Pair Encoding）BPE的大概训练过程：按照从左到右的顺序，将一个词拆分成多个子词，每个子词尽可能长。 greedy longest-match-first algorithm，贪婪最长优先匹配算法。<br><img src="bpe.png" alt="BPE"></p><h2 id="SELU激活函数"><a href="#SELU激活函数" class="headerlink" title="SELU激活函数"></a>SELU激活函数</h2><p>内部归一化的速度比外部归一化快，这意味着网络能更快收敛；<br>不可能出现梯度消失或爆炸问题，见 SELU 论文附录的定理 2 和 3。<br>$$<br>SELU(x) = \lambda x \quad if \quad  x&gt;0<br>$$<br>$$<br>SELU(x) = \lambda (\alpha e^x-\alpha) \quad if \quad  x&lt;=0<br>$$</p><h2 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h2><h3 id="Warm-up"><a href="#Warm-up" class="headerlink" title="Warm up"></a>Warm up</h3><p>在训练初期使用较小的学习率（从 0 开始），在一定步数（比如 1000 步）内逐渐提高到正常大小（比如上面的 2e-5），避免模型过早进入局部最优而过拟合；在训练后期再慢慢将学习率降低到 0，避免后期训练还出现较大的参数变化。</p><h3 id="标签平滑"><a href="#标签平滑" class="headerlink" title="标签平滑"></a>标签平滑</h3><p>和L1，L2，dropout一样的正则化方法，对one-hot标签向量平滑<br>$$<br>\hat y = y_{hot}(1-\alpha)+\alpha/K<br>$$<br>K是分类的个数；避免模型对于正确标签过于自信，使得预测正负样本的输出值差别不那么大，从而避免过拟合，提高模型的泛化能力。</p><p>标签平滑可以让分类之间的cluster更加紧凑，增加类间距离，减少类内距离，提高泛化性，同时还能提高Model Calibration（模型对于预测值的confidences和accuracies之间aligned的程度）。但是在模型蒸馏中使用Label smoothing会导致性能下降。</p><h3 id="gradient-checkpointing"><a href="#gradient-checkpointing" class="headerlink" title="gradient checkpointing"></a>gradient checkpointing</h3><p>用 gradient checkpointing 技术以降低训练时的显存占用。gradient checkpointing 即梯度检查点，通过减少保存的计算图节点压缩模型占用空间，但是在计算梯度的时候需要重新计算没有存储的值，参考论文《Training Deep Nets with Sublinear Memory Cost》</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>常见损失函数总结</title>
    <link href="/2023/03/04/%E5%B8%B8%E8%A7%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93/"/>
    <url>/2023/03/04/%E5%B8%B8%E8%A7%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h1 id="二分类交叉熵损失函数BCELoss"><a href="#二分类交叉熵损失函数BCELoss" class="headerlink" title="二分类交叉熵损失函数BCELoss"></a>二分类交叉熵损失函数BCELoss</h1><p>计算二分类任务时的交叉熵（Cross Entropy）函数。在二分类中，label是{0,1}。对于进入交叉熵函数的input为概率分布的形式。一般来说，input为sigmoid激活层的输出，或者softmax的输出。<br>$$<br>loss(x,y) = -y*log(x)-(1-y)*log(1-x)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br>m = nn.Sigmoid()<br>loss  = nn.BCELoss()<br>a = torch.randn(<span class="hljs-number">3</span>)<br>b = torch.empty(<span class="hljs-number">3</span>).random_(<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(loss(m(a),b)) <span class="hljs-comment"># 0.3618</span><br><br>c = m(a)<br><span class="hljs-built_in">print</span>(torch.mean(b*torch.log(c)+(<span class="hljs-number">1</span>-b)*torch.log(<span class="hljs-number">1</span>-c))) <span class="hljs-comment">#0.3618</span><br></code></pre></td></tr></table></figure><h1 id="交叉熵损失函数CrossEntropyLoss"><a href="#交叉熵损失函数CrossEntropyLoss" class="headerlink" title="交叉熵损失函数CrossEntropyLoss"></a>交叉熵损失函数CrossEntropyLoss</h1><p>$$<br>loss(x,class) = -log(\frac{e^{x[class]}}{\sum_{i=1}^{n}e^{x[i]}})= -x[class]+log(\sum_{i=1}^{n}e^{x[i]})<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">loss = nn.CrossEntropyLoss()<br><span class="hljs-built_in">input</span> = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, requires_grad=<span class="hljs-literal">True</span>)<br>target = torch.empty(<span class="hljs-number">3</span>, dtype=torch.long).random_(<span class="hljs-number">5</span>)<br>output = loss(<span class="hljs-built_in">input</span>, target)<br><span class="hljs-built_in">print</span>(output) <span class="hljs-comment">#1.57</span><br><br>tmp = torch.log(torch.<span class="hljs-built_in">sum</span>(torch.exp(<span class="hljs-built_in">input</span>),dim=<span class="hljs-number">1</span>))<br>new = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):<br>    new.append(<span class="hljs-built_in">input</span>[i][target[i]].item())<br>new = torch.mean(-torch.tensor(new)+tmp)<br><span class="hljs-built_in">print</span>(new) <span class="hljs-comment">#1.57</span><br></code></pre></td></tr></table></figure><h1 id="L1损失函数L1Loss"><a href="#L1损失函数L1Loss" class="headerlink" title="L1损失函数L1Loss"></a>L1损失函数L1Loss</h1><p>也称为Mean Absolute Error，即平均绝对误差(MAE),它衡量的是预测值与真实值之间距离的平均误差幅度，作用范围为0到正无穷。</p><ul><li>优点：无论对于什么样的输入值，都有着稳定的梯度，不会导致梯度爆炸问题，具有较为稳健性的解</li><li>缺点：在中心点是折点，不能求导，梯度下降时要是恰好学习到w=0就没法接着进行了</li></ul><p>同样的我们可以在一定的假设下通过最大化似然得到 MAE 损失的形式，假设模型预测与真实值之间的误差服从拉普拉斯分布 Laplace distribution（  ），则给定一个  模型输出真实值  的概率为</p><p>MAE 和 MSE 作为损失函数的主要区别是：MSE 损失相比 MAE 通常可以更快地收敛，但 MAE 损失对于 outlier 更加健壮，即更加不易受到 outlier 影响。</p><p>MSE 通常比 MAE 可以更快地收敛。当使用梯度下降算法时，MSE 损失的梯度为  ，而 MAE 损失的梯度为  ，即 MSE 的梯度的 scale 会随误差大小变化，而 MAE 的梯度的 scale 则一直保持为 1，即便在绝对误差  很小的时候 MAE 的梯度 scale 也同样为 1，这实际上是非常不利于模型的训练的。当然你可以通过在训练过程中动态调整学习率缓解这个问题，但是总的来说，损失函数梯度之间的差异导致了 MSE 在大部分时候比 MAE 收敛地更快。这个也是 MSE 更为流行的原因。</p><p>MAE 对于 outlier 更加 robust。我们可以从两个角度来理解这一点</p><p>$$<br>loss(x,y) = \frac{1}{n}\sum_{i=1}^{n}|x_i-y_i|<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br>x = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">5</span>,requires_grad = <span class="hljs-literal">True</span>)<br>y = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">5</span>)<br>loss = nn.L1Loss()<br>output = loss(x,y) <span class="hljs-comment">#调用函数</span><br><span class="hljs-built_in">print</span>(output) <span class="hljs-comment">#1.024</span><br><br><span class="hljs-comment">#复现</span><br>output_ = torch.mean(torch.<span class="hljs-built_in">abs</span>(x-y)) <span class="hljs-comment"># 1.024</span><br></code></pre></td></tr></table></figure><p>下面是L1Loss的LOSS图：<br><video id="video" controls="" preload="l1loss.mp4" poster="封面"><br>      <source id="mp4" src="l1loss.mp4" type="video/mp4"><br></videos></p><h1 id="L2损失函数-MSE损失函数MSELoss"><a href="#L2损失函数-MSE损失函数MSELoss" class="headerlink" title="L2损失函数/MSE损失函数MSELoss"></a>L2损失函数/MSE损失函数MSELoss</h1><p>均方误差（Mean Square Error,MSE），模型预测值f(x)和样本真实值y之间差值平方的平均值。</p><ul><li>优点：各点都连续光滑，方便求导，具有较为稳定的解；对离群点（Outliers）或者异常值更具有鲁棒性。</li><li>缺点：不是特别的稳健，因为当函数的输入值距离真实值较远的时候，对应loss值很大在两侧，则使用梯度下降法求解的时候梯度很大，可能导致梯度爆炸； 由图可知其在0点处的导数不连续，使得求解效率低下，导致收敛速度慢；而对于较小的损失值，其梯度也同其他区间损失值的梯度一样大，所以不利于网络的学习。</li></ul><p>可以看到这个实际上就是均方差损失的形式。也就是说在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的，因此在这个假设能被满足的场景中（比如回归），均方差损失是一个很好的损失函数选择；当这个假设没能被满足的场景中（比如分类），均方差损失不是一个好的选择。<a href="https://mp.weixin.qq.com/s?__biz=MzU0MDQ1NjAzNg==&mid=2247530581&idx=3&sn=08304b4b0f161a0a81538e1bfea27a2c&chksm=fb3add5ecc4d5448881026bd45798c60c6e2094899043a95830809996cc0611b6a2c70c53200&scene=27">链接</a><br>$$<br>loss(x,y) = \frac{1}{n}\sum_{i=1}^{n}(x_i-y_i)^2<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br>x = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">5</span>,requires_grad = <span class="hljs-literal">True</span>)<br>y = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">5</span>)<br>loss = nn.MSELoss()<br>output = loss(x,y) <span class="hljs-comment">#调用函数</span><br><span class="hljs-built_in">print</span>(output) <span class="hljs-comment">#1.9563</span><br><br><span class="hljs-comment">#复现</span><br>output_ = torch.mean(torch.<span class="hljs-built_in">abs</span>(x-y)**<span class="hljs-number">2</span>) <span class="hljs-comment"># 1.9563</span><br></code></pre></td></tr></table></figure><p>下面是L2Loss的LOSS图：<br><video id="video" controls="" preload="l2loss.mp4" poster="封面"><br>      <source id="mp4" src="l2loss.mp4" type="video/mp4"><br></videos></p><h1 id="平滑L1-Smooth-L1-损失函数SmoothL1Loss"><a href="#平滑L1-Smooth-L1-损失函数SmoothL1Loss" class="headerlink" title="平滑L1 (Smooth L1)损失函数SmoothL1Loss"></a>平滑L1 (Smooth L1)损失函数SmoothL1Loss</h1><p>L1的平滑输出，其功能是减轻离群点带来的影响。即平滑的L1损失（SLL）。SLL通过综合L1和L2损失的优点，在0点处附近采用了L2损失中的平方函数，解决了L1损失在0点处梯度不可导的问题，使其更加平滑易于收敛。此外，在|x|&gt;1的区间上，它又采用了L1损失中的线性函数，使得梯度能够快速下降。真实值和预测值差别较小时（绝对值差小于1），梯度也会比较小（损失函数比普通L1 loss在此处更圆滑），可以收敛得更快。真实值和预测值差别较大时，梯度值足够小（普通L2 loss在这种位置梯度值就很大，容易梯度爆炸）</p><!--$$loss(x,y) =  \left\{\begin{aligned} &\frac{1}{2}(x-y)^2  &\quad & if |x-y|<\delta\\ &\delta|x-y|-\frac{1}{2}\delta^2 & \quad & other\\\end{aligned}\right.$$--><p>$$<br>loss(x,y) = \frac{1}{2}(x-y)^2  …… if |x-y|&lt;\delta<br>$$<br>$$<br>loss(x,y) = \delta|x-y|-\frac{1}{2}\delta^2  …… other<br>$$</p><h2 id="三者区别"><a href="#三者区别" class="headerlink" title="三者区别"></a>三者区别</h2><ul><li>（1）L1 loss在零点不平滑，此处不可导，所以在w=0时没法接着梯度下降了，用的少</li><li>（2）L2 loss对离群点比较敏感，离群点处的梯度很大，容易梯度爆炸</li><li>（3）smooth L1 loss结合了L1和L2的优点，修改了零点不平滑问题，且比L2 loss对异常值的鲁棒性更强<h1 id="KL散度KLDivLoss"><a href="#KL散度KLDivLoss" class="headerlink" title="KL散度KLDivLoss"></a>KL散度KLDivLoss</h1>计算KL散度，也就是计算相对熵。用于连续分布的距离度量，并且对离散采用的连续输出空间分布进行回归通常很有用。<br>$$<br>KL(P,Q) = \sum_{i=1}^nP(x_i)(logP(x_i)-logQ(x_i))<br>$$<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs = torch.tensor([[<span class="hljs-number">0.5</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.2</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>]])<br>target = torch.tensor([[<span class="hljs-number">0.9</span>, <span class="hljs-number">0.05</span>, <span class="hljs-number">0.05</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.2</span>]], dtype=torch.<span class="hljs-built_in">float</span>)<br>loss = nn.KLDivLoss(reduction=<span class="hljs-string">&#x27;mean&#x27;</span>)<br>loss(inputs,target)<br></code></pre></td></tr></table></figure><img src="kl.png" alt="torch中KL实现"><h1 id="余弦相似度CosineEmbeddingLoss"><a href="#余弦相似度CosineEmbeddingLoss" class="headerlink" title="余弦相似度CosineEmbeddingLoss"></a>余弦相似度CosineEmbeddingLoss</h1>$$<br>cos(A,B) = \frac{A \cdot B}{||A||||B||}<br>$$<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">loss_f = nn.CosineEmbeddingLoss()<br>inputs_1 = torch.tensor([[<span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.7</span>], [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.7</span>]])<br>inputs_2 = torch.tensor([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>]])<br>target = torch.tensor([<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>], dtype=torch.<span class="hljs-built_in">float</span>)<br>output = loss_f(inputs_1,inputs_2,target)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;CosineEmbeddingLoss损失函数的计算结果为&#x27;</span>,output)<br></code></pre></td></tr></table></figure><img src="cos.png" alt="余弦相似度"></li></ul><h1 id="More…"><a href="#More…" class="headerlink" title="More…."></a>More….</h1>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch教程</title>
    <link href="/2023/03/04/PyTorch%E6%95%99%E7%A8%8B/"/>
    <url>/2023/03/04/PyTorch%E6%95%99%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="推荐一些比较实用的torch教程"><a href="#推荐一些比较实用的torch教程" class="headerlink" title="推荐一些比较实用的torch教程"></a>推荐一些比较实用的torch教程</h1><ul><li>深入浅出PyTorch <a href="https://github.com/datawhalechina/thorough-pytorch">链接</a></li><li>Pytorch模型训练实用教程 <a href="https://github.com/TingsongYu/PyTorch_Tutorial">链接</a></li><li>包含各个深度学习任务的torch教程 <a href="https://github.com/fendouai/PyTorchDocs">链接</a></li><li>小土堆的教程 <a href="https://github.com/xiaotudui/pytorch-tutorial">链接</a></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>TextCNN</title>
    <link href="/2023/03/03/TextCNN/"/>
    <url>/2023/03/03/TextCNN/</url>
    
    <content type="html"><![CDATA[<h1 id="TextCNN-代码＋图文对应解释"><a href="#TextCNN-代码＋图文对应解释" class="headerlink" title="TextCNN 代码＋图文对应解释"></a>TextCNN 代码＋图文对应解释</h1><h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><p>适合了解TextCNN大体思路，但是对Pytorch不了解的小伙伴阅读</p><h2 id="建议-遇到不懂的函数如squeeze-和unsqueeze-查阅官方文档，或者单独在百度上搜搜这个函数，你就懂了-如图"><a href="#建议-遇到不懂的函数如squeeze-和unsqueeze-查阅官方文档，或者单独在百度上搜搜这个函数，你就懂了-如图" class="headerlink" title="建议:遇到不懂的函数如squeeze()和unsqueeze(),查阅官方文档，或者单独在百度上搜搜这个函数，你就懂了,如图"></a>建议:遇到不懂的函数如squeeze()和unsqueeze(),查阅<a href="https://pytorch.org/docs/stable/search.html?q=squeeze&check_keywords=yes&area=default">官方文档</a>，或者单独在百度上搜搜这个函数，你就懂了,如图</h2><p><img src="https://img-blog.csdnimg.cn/20201216162011764.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h2><p><img src="https://img-blog.csdnimg.cn/20201216162359773.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="模型代码"><a href="#模型代码" class="headerlink" title="模型代码"></a>模型代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CNN</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(CNN,self).__init__()<br>        <span class="hljs-comment"># 设置embeding层</span><br>        self.embedding_choice=embedding_choice<br>        <span class="hljs-keyword">if</span> self.embedding_choice==  <span class="hljs-string">&#x27;rand&#x27;</span>:<br>            self.embedding=nn.Embedding(num_embeddings,embedding_dim)<br>        <span class="hljs-keyword">if</span> self.embedding_choice==  <span class="hljs-string">&#x27;glove&#x27;</span>:<br>            self.embedding = nn.Embedding(num_embeddings, embedding_dim, <br>                padding_idx=PAD_INDEX).from_pretrained(TEXT.vocab.vectors, freeze=<span class="hljs-literal">True</span>)<br>       <span class="hljs-comment">#三层卷积层设置 convolution</span><br>        self.conv1 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                               kernel_size=(<span class="hljs-number">3</span>, embedding_dim), padding=(<span class="hljs-number">2</span>,<span class="hljs-number">0</span>))<br>        <br>        self.conv2 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                               kernel_size=(<span class="hljs-number">4</span>, embedding_dim), padding=(<span class="hljs-number">3</span>,<span class="hljs-number">0</span>))<br>        <br>        self.conv3 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                               kernel_size=(<span class="hljs-number">5</span>, embedding_dim), padding=(<span class="hljs-number">4</span>,<span class="hljs-number">0</span>))<br>        <span class="hljs-comment"># 正则化，防止过拟合</span><br>        self.dropout = nn.Dropout(dropout_p)<br>        <span class="hljs-comment">#全连接层 functional connect</span><br>        self.fc = nn.Linear(filters_num * <span class="hljs-number">3</span>, label_num)<br>       <span class="hljs-comment"># 前向传播</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):      <span class="hljs-comment"># (Batch_size, Length) </span><br>        x=self.embedding(x).unsqueeze(<span class="hljs-number">1</span>)      <span class="hljs-comment">#(Batch_size, Length, Dimention) </span><br>                                       <span class="hljs-comment">#(Batch_size, 1, Length, Dimention) </span><br>        <span class="hljs-comment"># Relu函数 提供非线性</span><br>        x1 = F.relu(self.conv1(x)).squeeze(<span class="hljs-number">3</span>)    <span class="hljs-comment">#(Batch_size, filters_num, length+padding, 1) </span><br>                                          <span class="hljs-comment">#(Batch_size, filters_num, length+padding) </span><br>        <span class="hljs-comment"># 池化层-降维</span><br>        x1 = F.max_pool1d(x1, x1.size(<span class="hljs-number">2</span>)).squeeze(<span class="hljs-number">2</span>)  <span class="hljs-comment">#(Batch_size, filters_num, 1)</span><br>                                               <span class="hljs-comment">#(Batch_size, filters_num) </span><br>         <br>        x2 = F.relu(self.conv2(x)).squeeze(<span class="hljs-number">3</span>)  <br>        x2 = F.max_pool1d(x2, x2.size(<span class="hljs-number">2</span>)).squeeze(<span class="hljs-number">2</span>)      <br>        <br>        x3 = F.relu(self.conv3(x)).squeeze(<span class="hljs-number">3</span>)  <br>        x3 = F.max_pool1d(x3, x3.size(<span class="hljs-number">2</span>)).squeeze(<span class="hljs-number">2</span>)      <br>        <br>        <span class="hljs-comment"># 拼接</span><br>        x = torch.cat((x1, x2, x3), dim=<span class="hljs-number">1</span>)  <span class="hljs-comment">#(Batch_size, filters_num *3 )</span><br>        x = self.dropout(x)      <span class="hljs-comment">#(Batch_size, filters_num *3 )</span><br>        out = self.fc(x)       <span class="hljs-comment">#(Batch_size, label_num  )</span><br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><h2 id="设置-Embedding-层"><a href="#设置-Embedding-层" class="headerlink" title="设置 Embedding 层"></a>设置 Embedding 层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">self.embedding_choice=embedding_choice<br>       <span class="hljs-keyword">if</span> self.embedding_choice==  <span class="hljs-string">&#x27;rand&#x27;</span>:<br>           self.embedding=nn.Embedding(num_embeddings,embedding_dim)<br>       <span class="hljs-keyword">if</span> self.embedding_choice==  <span class="hljs-string">&#x27;glove&#x27;</span>:<br>           self.embedding = nn.Embedding(num_embeddings, embedding_dim, <br>               padding_idx=PAD_INDEX).from_pretrained(TEXT.vocab.vectors, freeze=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>其中 <strong>rand模式</strong> 就是每个词向量随机赋值，<strong>glove模式</strong>就是从预训练的glove词向量中选择，如果这个词在glove中没有，就按照均匀分布给他随机赋值，<strong>padding_idx</strong> 参数为指定某行全为0</p><p><img src="https://img-blog.csdnimg.cn/20201216161215215.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="设置卷积层"><a href="#设置卷积层" class="headerlink" title="设置卷积层"></a>设置卷积层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#三层卷积层设置 convolution</span><br> self.conv1 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                        kernel_size=(<span class="hljs-number">3</span>, embedding_dim), padding=(<span class="hljs-number">2</span>,<span class="hljs-number">0</span>))<br> <br> self.conv2 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                        kernel_size=(<span class="hljs-number">4</span>, embedding_dim), padding=(<span class="hljs-number">3</span>,<span class="hljs-number">0</span>))<br> <br> self.conv3 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>,out_channels=filters_num ,  <span class="hljs-comment">#卷积产生的通道</span><br>                        kernel_size=(<span class="hljs-number">5</span>, embedding_dim), padding=(<span class="hljs-number">4</span>,<span class="hljs-number">0</span>))<br></code></pre></td></tr></table></figure><p><strong>padding是在外围补多少0</strong></p><h3 id="如图"><a href="#如图" class="headerlink" title="如图"></a>如图</h3><p><img src="https://img-blog.csdnimg.cn/20201216161305642.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h2><p><img src="https://img-blog.csdnimg.cn/2020121616145068.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">x1 = F.max_pool1d(x1, x1.size(<span class="hljs-number">2</span>)).squeeze(<span class="hljs-number">2</span>)  <span class="hljs-comment">#(Batch_size, filters_num, 1)</span><br>                                               <span class="hljs-comment">#(Batch_size, filters_num) </span><br></code></pre></td></tr></table></figure><h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 拼接</span><br>x = torch.cat((x1, x2, x3), dim=<span class="hljs-number">1</span>)  <span class="hljs-comment">#(Batch_size, filters_num *3 )</span><br>x = self.dropout(x)      <span class="hljs-comment">#(Batch_size, filters_num *3 )</span><br>out = self.fc(x)       <span class="hljs-comment">#(Batch_size, label_num  )</span><br></code></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20201216161642843.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNzI2MTgz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch并行训练</title>
    <link href="/2023/03/03/PyTorch%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/"/>
    <url>/2023/03/03/PyTorch%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/</url>
    
    <content type="html"><![CDATA[<h2 id="下面代码实现torch多卡同时训练"><a href="#下面代码实现torch多卡同时训练" class="headerlink" title="下面代码实现torch多卡同时训练"></a>下面代码实现torch多卡同时训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pytorch <span class="hljs-keyword">as</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data.distributed <span class="hljs-keyword">import</span> DistributedSample<br>torch.distributed.init_process_group(backend=<span class="hljs-string">&#x27;nccl&#x27;</span>)<span class="hljs-comment">#初始化并行训练</span><br>n_gpu = torch.cuda.device_count()<span class="hljs-comment">#统计gpu数量</span><br>model = torch.nn.DataParallel(model)<span class="hljs-comment">#多卡部署模型</span><br>data  = TensorDataset(data)<br>data = DistributedSample(data)<span class="hljs-comment">#分布训练</span><br>loss = model(data)<br>loss = loss.mean()<span class="hljs-comment">#多卡取平均loss</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
